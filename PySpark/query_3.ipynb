{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db1176b0-c23b-4970-9406-24fd018c7fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3454</td><td>application_1732639283265_3410</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3410/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3410_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeoJSON read\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Read the file from s3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "# Formatting magic\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\") \\\n",
    "            .filter(col(\"CITY\") == \"Los Angeles\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6a2c1ab-7027-4a3d-a118-5684680f1406",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Instances: None\n",
      "Executor Memory: 4743M\n",
      "Executor Cores: 2\n",
      "+--------------------+----------+-----------------------+\n",
      "|                COMM|Crime rate|Estimated_Median_Income|\n",
      "+--------------------+----------+-----------------------+\n",
      "|     Adams-Normandie|    0.7149|                 $8,791|\n",
      "|              Alsace|    0.5416|                $11,240|\n",
      "|Angeles National ...|    0.4118|                $33,080|\n",
      "|    Angelino Heights|    0.5733|                $18,427|\n",
      "|              Arleta|    0.4265|                $12,111|\n",
      "|     Atwater Village|    0.5288|                $28,481|\n",
      "|       Baldwin Hills|    0.9950|                $17,304|\n",
      "|             Bel Air|    0.3992|                $63,041|\n",
      "|       Beverly Crest|    0.3690|                $60,947|\n",
      "|         Beverlywood|    0.5085|                $29,268|\n",
      "|       Boyle Heights|    0.6172|                 $8,494|\n",
      "|           Brentwood|    0.4059|                $60,847|\n",
      "|           Brookside|    0.8857|                $18,139|\n",
      "|    Cadillac-Corning|    0.5817|                $19,573|\n",
      "|         Canoga Park|    0.5506|                $19,660|\n",
      "|             Carthay|    0.7629|                $49,849|\n",
      "|             Central|    0.6594|                 $6,973|\n",
      "|        Century City|    0.6330|                $45,618|\n",
      "|  Century Palms/Cove|    1.1446|                 $8,610|\n",
      "|          Chatsworth|    0.5281|                $30,695|\n",
      "+--------------------+----------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time : 25.31273651123047 seconds"
     ]
    }
   ],
   "source": [
    "## Solution using Dataframe\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import date_format, to_timestamp, round, format_number, concat, lit ,when, col, count , sum ,regexp_replace ,trim\n",
    "from pyspark.sql.types import FloatType,IntegerType\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Query 2 DF\").getOrCreate()\n",
    "spark.catalog.clearCache()\n",
    "conf = spark.sparkContext.getConf()\n",
    "\n",
    "# Print relevant executor settings\n",
    "print(\"Executor Instances:\", conf.get(\"spark.executor.instances\"))\n",
    "print(\"Executor Memory:\", conf.get(\"spark.executor.memory\"))\n",
    "print(\"Executor Cores:\", conf.get(\"spark.executor.cores\"))\n",
    "\n",
    "csv_files = [\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "]\n",
    "\n",
    "crime_data_df = spark.read.csv(csv_files, header=True, inferSchema=True)\n",
    "income_data = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True, inferSchema=True)\n",
    "# convert median income to int\n",
    "start_time = time.time()\n",
    "income_data_cleaned = income_data.withColumn(\n",
    "    \"Estimated_Median_Income\",\n",
    "    regexp_replace(trim(col(\"Estimated Median Income\")), \"[^0-9]\", \"\").cast(IntegerType())\n",
    "    ).drop(\"Community\",\"Estimated Median Income\")\n",
    "\n",
    "#Filter basic dataset and find total population and total number of houses\n",
    "zip_comm_population = flattened_df.select(\"ZCTA10\",\"COMM\",\"POP_2010\",\"HOUSING10\",\"geometry\").filter((col(\"ZCTA10\")>0) & ((col(\"POP_2010\")>0) & (col(\"HOUSING10\")>0)) & (trim(col(\"COMM\"))!=\"\")) \\\n",
    "                    .groupBy(\"ZCTA10\",\"COMM\").agg(sum(\"POP_2010\").alias(\"Total_Population_Zip_COMM\"),sum(\"HOUSING10\").alias(\"Total_Housing\"),ST_Union_Aggr(\"geometry\").alias(\"geometry\"))\n",
    "\n",
    "# join income data and census blocks data using zip code\n",
    "# then calculate total income,total population and Estimated_Median_Income for each comm \n",
    "joined_income_dataset = zip_comm_population.join(\n",
    "    income_data_cleaned,\n",
    "    zip_comm_population[\"ZCTA10\"] == income_data_cleaned[\"Zip Code\"],\n",
    "    \"inner\"  \n",
    "    ) \\\n",
    "    .drop(\"ZCTA10\",\"Zip Code\") \\\n",
    "    .withColumn(\"Total income\",col(\"Total_Housing\") * col(\"Estimated_Median_Income\")) \\\n",
    "    .drop(\"Estimated_Median_Income\") \\\n",
    "    .groupBy(\"COMM\") \\\n",
    "    .agg(sum(\"Total_Population_Zip_COMM\").alias(\"Total_Population\"),sum(\"Total income\").alias(\"Total Income\"),ST_Union_Aggr(\"geometry\").alias(\"geometry\")) \n",
    "crime_data_with_geometry = crime_data_df.withColumn(\"geom\", ST_Point(\"LON\", \"LAT\")).select(\"geom\")\n",
    "final_dataset = joined_income_dataset.join(\n",
    "                crime_data_with_geometry, \n",
    "                ST_Within(crime_data_with_geometry.geom, \n",
    "                joined_income_dataset.geometry), \"inner\") \\\n",
    "                .groupBy(\"COMM\",\"Total_Population\",\"Total Income\") \\\n",
    "                .agg(count(col(\"*\")).alias(\"Total crimes\")) \\\n",
    "                .withColumn(\"Crime rate\",format_number((col(\"Total crimes\")/col(\"Total_Population\")),4)) \\\n",
    "                .withColumn(\"Estimated_Median_Income\",concat(lit(\"$\"),format_number(round(col(\"Total Income\") / col(\"Total_Population\")), 0))) \\\n",
    "                .drop(\"Total Income\",\"Total_Population\",\"Total crimes\") \\\n",
    "                .orderBy(\"COMM\")\n",
    "final_dataset.show() \n",
    "end_time = time.time()\n",
    "# End timing\n",
    "print(f\"Execution Time : {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b06db5d0-f289-435b-bbdf-ede8a38fa820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (29)\n",
      "+- Sort (28)\n",
      "   +- Exchange (27)\n",
      "      +- HashAggregate (26)\n",
      "         +- Exchange (25)\n",
      "            +- HashAggregate (24)\n",
      "               +- Project (23)\n",
      "                  +- RangeJoin (22)\n",
      "                     :- Filter (18)\n",
      "                     :  +- ObjectHashAggregate (17)\n",
      "                     :     +- Exchange (16)\n",
      "                     :        +- ObjectHashAggregate (15)\n",
      "                     :           +- Project (14)\n",
      "                     :              +- BroadcastHashJoin Inner BuildRight (13)\n",
      "                     :                 :- ObjectHashAggregate (8)\n",
      "                     :                 :  +- Exchange (7)\n",
      "                     :                 :     +- ObjectHashAggregate (6)\n",
      "                     :                 :        +- Project (5)\n",
      "                     :                 :           +- Filter (4)\n",
      "                     :                 :              +- Generate (3)\n",
      "                     :                 :                 +- Filter (2)\n",
      "                     :                 :                    +- Scan geojson  (1)\n",
      "                     :                 +- BroadcastExchange (12)\n",
      "                     :                    +- Project (11)\n",
      "                     :                       +- Filter (10)\n",
      "                     :                          +- Scan csv  (9)\n",
      "                     +- Project (21)\n",
      "                        +- Filter (20)\n",
      "                           +- Scan csv  (19)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#25]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#25]\n",
      "Condition : ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#25]\n",
      "Arguments: explode(features#25), false, [features#33]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#33]\n",
      "Condition : ((((isnotnull(features#33.properties.CITY) AND isnotnull(features#33.properties.ZCTA10)) AND isnotnull(features#33.properties.POP_2010)) AND isnotnull(features#33.properties.HOUSING10)) AND ((features#33.properties.CITY = Los Angeles) AND ((((cast(features#33.properties.ZCTA10 as int) > 0) AND (features#33.properties.POP_2010 > 0)) AND (features#33.properties.HOUSING10 > 0)) AND NOT (trim(features#33.properties.COMM, None) = ))))\n",
      "\n",
      "(5) Project\n",
      "Output [5]: [features#33.properties.ZCTA10 AS ZCTA10#66, features#33.properties.COMM AS COMM#49, features#33.properties.POP_2010 AS POP_2010#58L, features#33.properties.HOUSING10 AS HOUSING10#55L, features#33.geometry AS geometry#36]\n",
      "Input [1]: [features#33]\n",
      "\n",
      "(6) ObjectHashAggregate\n",
      "Input [5]: [ZCTA10#66, COMM#49, POP_2010#58L, HOUSING10#55L, geometry#36]\n",
      "Keys [2]: [ZCTA10#66, COMM#49]\n",
      "Functions [3]: [partial_sum(POP_2010#58L), partial_sum(HOUSING10#55L), partial_st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@28427fd1, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#459L, sum#461L, buf#463]\n",
      "Results [5]: [ZCTA10#66, COMM#49, sum#460L, sum#462L, buf#464]\n",
      "\n",
      "(7) Exchange\n",
      "Input [5]: [ZCTA10#66, COMM#49, sum#460L, sum#462L, buf#464]\n",
      "Arguments: hashpartitioning(ZCTA10#66, COMM#49, 1000), ENSURE_REQUIREMENTS, [plan_id=678]\n",
      "\n",
      "(8) ObjectHashAggregate\n",
      "Input [5]: [ZCTA10#66, COMM#49, sum#460L, sum#462L, buf#464]\n",
      "Keys [2]: [ZCTA10#66, COMM#49]\n",
      "Functions [3]: [sum(POP_2010#58L), sum(HOUSING10#55L), st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@28427fd1, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(POP_2010#58L)#286L, sum(HOUSING10#55L)#288L, ST_Union_Aggr(geometry#36)#293]\n",
      "Results [5]: [ZCTA10#66, COMM#49, sum(POP_2010#58L)#286L AS Total_Population_Zip_COMM#287L, sum(HOUSING10#55L)#288L AS Total_Housing#289L, ST_Union_Aggr(geometry#36)#293 AS geometry#294]\n",
      "\n",
      "(9) Scan csv \n",
      "Output [2]: [Zip Code#262, Estimated Median Income#264]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "\n",
      "(10) Filter\n",
      "Input [2]: [Zip Code#262, Estimated Median Income#264]\n",
      "Condition : isnotnull(Zip Code#262)\n",
      "\n",
      "(11) Project\n",
      "Output [2]: [Zip Code#262, cast(regexp_replace(trim(Estimated Median Income#264, None), [^0-9], , 1) as int) AS Estimated_Median_Income#268]\n",
      "Input [2]: [Zip Code#262, Estimated Median Income#264]\n",
      "\n",
      "(12) BroadcastExchange\n",
      "Input [2]: [Zip Code#262, Estimated_Median_Income#268]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=681]\n",
      "\n",
      "(13) BroadcastHashJoin\n",
      "Left keys [1]: [cast(ZCTA10#66 as int)]\n",
      "Right keys [1]: [Zip Code#262]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(14) Project\n",
      "Output [4]: [COMM#49, Total_Population_Zip_COMM#287L, geometry#294, (Total_Housing#289L * cast(Estimated_Median_Income#268 as bigint)) AS Total income#331L]\n",
      "Input [7]: [ZCTA10#66, COMM#49, Total_Population_Zip_COMM#287L, Total_Housing#289L, geometry#294, Zip Code#262, Estimated_Median_Income#268]\n",
      "\n",
      "(15) ObjectHashAggregate\n",
      "Input [4]: [COMM#49, Total_Population_Zip_COMM#287L, geometry#294, Total income#331L]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [3]: [partial_sum(Total_Population_Zip_COMM#287L), partial_sum(Total income#331L), partial_st_union_aggr(geometry#294, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@3804824e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#453L, sum#455L, buf#457]\n",
      "Results [4]: [COMM#49, sum#454L, sum#456L, buf#458]\n",
      "\n",
      "(16) Exchange\n",
      "Input [4]: [COMM#49, sum#454L, sum#456L, buf#458]\n",
      "Arguments: hashpartitioning(COMM#49, 1000), ENSURE_REQUIREMENTS, [plan_id=686]\n",
      "\n",
      "(17) ObjectHashAggregate\n",
      "Input [4]: [COMM#49, sum#454L, sum#456L, buf#458]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [3]: [sum(Total_Population_Zip_COMM#287L), sum(Total income#331L), st_union_aggr(geometry#294, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@3804824e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(Total_Population_Zip_COMM#287L)#348L, sum(Total income#331L)#350L, ST_Union_Aggr(geometry#294)#355]\n",
      "Results [4]: [COMM#49, sum(Total_Population_Zip_COMM#287L)#348L AS Total_Population#349L, sum(Total income#331L)#350L AS Total Income#351L, ST_Union_Aggr(geometry#294)#355 AS geometry#356]\n",
      "\n",
      "(18) Filter\n",
      "Input [4]: [COMM#49, Total_Population#349L, Total Income#351L, geometry#356]\n",
      "Condition : isnotnull(geometry#356)\n",
      "\n",
      "(19) Scan csv \n",
      "Output [2]: [LAT#214, LON#215]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv, ... 1 entries]\n",
      "ReadSchema: struct<LAT:string,LON:string>\n",
      "\n",
      "(20) Filter\n",
      "Input [2]: [LAT#214, LON#215]\n",
      "Condition : isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "\n",
      "(21) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#373]\n",
      "Input [2]: [LAT#214, LON#215]\n",
      "\n",
      "(22) RangeJoin\n",
      "Arguments: geometry#356: geometry, geom#373: geometry, CONTAINS\n",
      "\n",
      "(23) Project\n",
      "Output [3]: [COMM#49, Total_Population#349L, Total Income#351L]\n",
      "Input [5]: [COMM#49, Total_Population#349L, Total Income#351L, geometry#356, geom#373]\n",
      "\n",
      "(24) HashAggregate\n",
      "Input [3]: [COMM#49, Total_Population#349L, Total Income#351L]\n",
      "Keys [3]: [COMM#49, Total_Population#349L, Total Income#351L]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#451L]\n",
      "Results [4]: [COMM#49, Total_Population#349L, Total Income#351L, count#452L]\n",
      "\n",
      "(25) Exchange\n",
      "Input [4]: [COMM#49, Total_Population#349L, Total Income#351L, count#452L]\n",
      "Arguments: hashpartitioning(COMM#49, Total_Population#349L, Total Income#351L, 1000), ENSURE_REQUIREMENTS, [plan_id=693]\n",
      "\n",
      "(26) HashAggregate\n",
      "Input [4]: [COMM#49, Total_Population#349L, Total Income#351L, count#452L]\n",
      "Keys [3]: [COMM#49, Total_Population#349L, Total Income#351L]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#420L]\n",
      "Results [3]: [COMM#49, format_number((cast(count(1)#420L as double) / cast(Total_Population#349L as double)), 4) AS Crime rate#426, concat($, format_number(round((cast(Total Income#351L as double) / cast(Total_Population#349L as double)), 0), 0)) AS Estimated_Median_Income#432]\n",
      "\n",
      "(27) Exchange\n",
      "Input [3]: [COMM#49, Crime rate#426, Estimated_Median_Income#432]\n",
      "Arguments: rangepartitioning(COMM#49 ASC NULLS FIRST, 1000), ENSURE_REQUIREMENTS, [plan_id=696]\n",
      "\n",
      "(28) Sort\n",
      "Input [3]: [COMM#49, Crime rate#426, Estimated_Median_Income#432]\n",
      "Arguments: [COMM#49 ASC NULLS FIRST], true, 0\n",
      "\n",
      "(29) AdaptiveSparkPlan\n",
      "Output [3]: [COMM#49, Crime rate#426, Estimated_Median_Income#432]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "# use explain to get the physican plan of the catalyst optimizer\n",
    "final_dataset.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba086d7b-6109-43bf-9869-784ff205c697",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income_data_cleaned number of columns: 2\n",
      "Income_data_cleaned number of entries: 284"
     ]
    }
   ],
   "source": [
    "# For the report to justify the use of broadcast join by the catalyst optimizer\n",
    "num_rows = income_data_cleaned.count()\n",
    "num_columns = len(income_data_cleaned.columns)\n",
    "print(f\"Income_data_cleaned number of columns: {num_columns}\")\n",
    "print(f\"Income_data_cleaned number of entries: {num_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a7fffe6-8b4f-49fb-a9ce-b4430cd7b65a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "import time\n",
    "#the join strategies tested are the following:\n",
    "#join_strategies = [\"SHUFFLE_REPLICATE_NL\",\"MERGE\",\"BROADCAST\", \"SHUFFLE_HASH\"]\n",
    "\n",
    "results = []\n",
    "first_join_strategy = \"SHUFFLE_HASH\"\n",
    "second_join_strategy = \"SHUFFLE_HASH\"\n",
    "# We apply the hint to set the join strategy used\n",
    "# the code executes the same query as presented above\n",
    "spark.catalog.clearCache()\n",
    "start_time = time.time()\n",
    "joined_income_dataset = zip_comm_population.hint(first_join_strategy).join(\n",
    "    income_data_cleaned.hint(first_join_strategy),\n",
    "    zip_comm_population[\"ZCTA10\"] == income_data_cleaned[\"Zip Code\"],\n",
    "    \"inner\"\n",
    ") \\\n",
    ".drop(\"ZCTA10\", \"Zip Code\") \\\n",
    ".withColumn(\"Total income\", col(\"Total_Housing\") * col(\"Estimated_Median_Income\")) \\\n",
    ".drop(\"Estimated_Median_Income\") \\\n",
    ".groupBy(\"COMM\") \\\n",
    ".agg(\n",
    "    sum(\"Total_Population_Zip_COMM\").alias(\"Total_Population\"),\n",
    "    sum(\"Total income\").alias(\"Total Income\"),\n",
    "    ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    ")\n",
    "\n",
    "crime_data_with_geometry = crime_data_df.withColumn(\"geom\", ST_Point(\"LON\", \"LAT\")).select(\"geom\")\n",
    "\n",
    "final_dataset = joined_income_dataset.hint(second_join_strategy).join(\n",
    "    crime_data_with_geometry.hint(second_join_strategy),\n",
    "    ST_Within(crime_data_with_geometry.geom, joined_income_dataset.geometry),\n",
    "    \"inner\"\n",
    ") \\\n",
    ".groupBy(\"COMM\", \"Total_Population\", \"Total Income\") \\\n",
    ".agg(count(col(\"*\")).alias(\"Total crimes\")) \\\n",
    ".withColumn(\"Crime rate\", format_number((col(\"Total crimes\") / col(\"Total_Population\")), 4)) \\\n",
    ".withColumn(\n",
    "    \"Estimated_Median_Income\",\n",
    "    concat(lit(\"$\"), format_number(round(col(\"Total Income\") / col(\"Total_Population\")), 0))\n",
    ") \\\n",
    ".drop(\"Total Income\") \\\n",
    ".orderBy(\"COMM\")\n",
    "\n",
    "final_dataset.collect() \n",
    "end_time = time.time()\n",
    "# Store the result\n",
    "results.append(Row(\n",
    "    First_Join_Strategy=first_join_strategy,\n",
    "    Second_Join_Strategy=second_join_strategy,\n",
    "    Execution_Time=end_time - start_time\n",
    "))\n",
    "\n",
    "results_df = spark.createDataFrame(results)\n",
    "\n",
    "# Save the DataFrame to S3 as JSON\n",
    "# remember, set write mode to overwrite the first time you run this in order to overwrite the current result file\n",
    "# if you dont do this you will get duplicate entries\n",
    "# after the first execution se it back to append mode\n",
    "results_df.write.mode(\"overwrite\").json(\"s3://groups-bucket-dblab-905418150721/group45/q3/res.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "491f3a06-cdf0-4e01-a173-4c36183c0770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+\n",
      "|First_Join_Strategy |Second_Join_Strategy|Execution_Time|\n",
      "+--------------------+--------------------+--------------+\n",
      "|BROADCAST           |SHUFFLE_HASH        |18.962        |\n",
      "|SHUFFLE_HASH        |MERGE               |20.181        |\n",
      "|SHUFFLE_REPLICATE_NL|BROADCAST           |20.278        |\n",
      "|BROADCAST           |MERGE               |20.745        |\n",
      "|SHUFFLE_REPLICATE_NL|SHUFFLE_REPLICATE_NL|21.029        |\n",
      "|MERGE               |SHUFFLE_HASH        |21.050        |\n",
      "|SHUFFLE_HASH        |SHUFFLE_HASH        |21.169        |\n",
      "|BROADCAST           |SHUFFLE_REPLICATE_NL|21.249        |\n",
      "|SHUFFLE_REPLICATE_NL|MERGE               |21.499        |\n",
      "|SHUFFLE_HASH        |BROADCAST           |21.720        |\n",
      "|SHUFFLE_HASH        |SHUFFLE_REPLICATE_NL|22.807        |\n",
      "|MERGE               |SHUFFLE_REPLICATE_NL|25.419        |\n",
      "|BROADCAST           |BROADCAST           |25.811        |\n",
      "|MERGE               |BROADCAST           |26.330        |\n",
      "|MERGE               |MERGE               |28.349        |\n",
      "|SHUFFLE_REPLICATE_NL|SHUFFLE_HASH        |32.894        |\n",
      "+--------------------+--------------------+--------------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, format_number\n",
    "#get the results\n",
    "results_df = spark.read.json(\"s3://groups-bucket-dblab-905418150721/group45/q3/results.json\")\n",
    "results_df.select(col(\"First_Join_Strategy\"),col(\"Second_Join_Strategy\"),format_number(col(\"Execution_Time\"), 3).alias(\"Execution_Time\")).orderBy(\"Execution_Time\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7948244-8475-4c32-9403-1dfb914e6f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
